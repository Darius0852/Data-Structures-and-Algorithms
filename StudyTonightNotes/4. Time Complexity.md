# Time Complexity of Algorithms

For any defined problem. there can be N number of solutions. This is true in general. 
If I have a problem and I discuss about the problem with all of my freinds, they will 
all suggest me different solutions. And I am the one who has to decide which solution 
is the best based on the circumstances.

Similaroty for any problem which must be solved using a program, there can be infinite
number of solutions. Let's take a simple example to understand this. Below we have two different 
algorithms to find the square of a number (for some time, forget that square of any number n is n x n):

One solution to this problem can be, running a loop for n times, starting with the number n and adding n to it,
every time. 

```
/* 
    we have to calculate the square of n
*/
for i=1 to n
    do n = n + n
// when the loop ends n will hold its square
return n
```

Or, we can simply use a mathematical operator * to find the square.
```
/* 
    we have to calculate the square of n
*/
return n*n
```
In the above two algorithms, you saw how a single problem can have many solutions.
While the first solution required a loop which will execute for n number of times, the second solution used a mathematical operator to return the result in one line. So which one is the better approach, of course the second one.


## What is Time Complexity

Time complexity of an algorithm signifies the totasl time required by the program to run till its completion.

The time complexity of algorithms is most commonly expressed using the **big O notation**.
It's an aymptotic notation to represent the time complexity.

Time Complexity is most commonly esitmated by counting the number of elementary steps performed by any algoithm to finish its execution. Like in the example above, for the first code the loop will run n number of times, so the time complexity will be n at least and as the value of n will increase the time taken will also increase. While for the second code, time complexity is constant, because it will never be dependant on the value of n, it will always give the result in 1 step.

And since the algorithm's performance may vary with different types of input data, hence for an algorithm we usually use the **worst-case Time Complexity** of an algorithm because that is the maximum time taken for any input size. 

## Calculating Time Complexity

Now lets tap onto the next big topic related to Time Complexity, which is how to calculate Time Complexity. It becaomes very confusing some times, but we will try to exaplain it simply.

Now the most common metric for calculating time complexity is Big O notation. This remoces all constant factors so that the running time can be estimated in relation to **N**, as N approaches infinity. In general, you can think of it like this:

```
STATEMENT;
```
Above we have a single statement. It's Time Complexity will be **COnstant**.
The running time of the statement will. not change in relation to N.
```
for (i=0; i < N; i++)
{
    statement;
}
```

The time complexity for the above algorithm will be **Linear**. The running time of the loop is directly proportional to N. When N doubles, so does the running time.

```
for(i=0; i < N; i++) 
{
    for(j=0; j < N;j++)
    { 
    statement;
    }
}
```
This time, the time complexity for the above code will be **Quadratic**. The running time of the two loops is proportional to the square of N. When N doubles, the running time increases by N x N.

```
while(low <= high) 
{
    mid = (low + high) / 2;
    if (target < list[mid])
        high = mid - 1;
    else if (target > list[mid])
        low = mid + 1;
    else break;
}
```

This is an algorithm to break a set of numbers into halves, to search a particular field. Now, this algorithm will have a **Logarithmic** Time Complexity. The running time of the algorithm is proportional to the number of times N can be divided by 2(N is high-low here). This is because the algorithm divides the working area in half with each iteration.

```
void quicksort(int list[], int left, int right)
{
    int pivot = partition(list, left, right);
    quicksort(list, left, pivot - 1);
    quicksort(list, pivot + 1, right);
}
```

Taking the previous algorithm forward, above we have a small example of **Quick Sort** logic. Now in quick sort, we divide the list into halves every time, but we repeat the iteration N times (where N is the size of the list). Hence time complexity will be **N x log(N)**. The running time consists of N loops (iterative or recursive) that are logarithmic, thus the algorithm is a combination of linear and logarithmic.

NOTE: In general, doing something with every item in one dimension is linear, doing something with every item in two dimensions is quadratic, and dividing the working area in half is logarithmic.

## Types of Notations for Time Complexity

Now we will discuss and understand the various notations for Time Complexity.
1. **Big Oh** denotes *"fewer than or the same as"* <expression> iterations.
2. **Big Omega** denotes *"more than or the same as"* <expression> iterations.
3. **Big Theta** denotes *"the same as"* <expression> iterations.
4. **Little Oh** denotes *"fewer than"* <expression iterations.
5. **Little Omega** denotes *"more than"* <expression> iterations. 

## Understanding Notations of Time Complexity with Example

**O (expression)** is the set of cuntions that grow slower than or at the same rate as expression. It indicates the maximum required by an algorithm for all input values. It respresents the worst case of an algorithm's time complexity. 

**Omega (expression)** is the set of functions that grow faster than or at the same rate as expression. It indicates the minimum time requires by an algorithm for all input values. It represents the best case of an algorithm's time complexity.

**Theta (expression)** consists of all the functions that lie in both O (expression) and Omega (expression). It indicates the average bound of an algorithm. It represents the average case of an algorithm's time complexity. 

Supoose you've calculated that an algorith, takes f(n) operations, where,
```
f(n) = 3*n^2 + 2*n + 4.   // n^2 means square of n
```
Since this polynomial grows at the same rate as **n squared**, then you could say that the function f lies in the set **Theta(n squared)**. (It also lies in the sets **O(n squared)** and **Omega(n squared)** for the same reason.)


























